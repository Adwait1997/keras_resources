{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-85b569bde3af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#exponential decay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschedules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExponentialDecay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_learning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecay_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecay_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstaircase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#during the training of a model the learning rate is recommended to be reduces as the training progresses and the loss as reduces as well. The schedule is an algorithm that applies an exponential decay to an optimizer step given an initial learning rate.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "#exponential decay\n",
    "\n",
    "tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate, decay_steps, decay_rate, staircase=False, name=None)\n",
    "\n",
    "#during the training of a model the learning rate is recommended to be reduces as the training progresses and the loss as reduces as well. The schedule is an algorithm that applies an exponential decay to an optimizer step given an initial learning rate.\n",
    "\n",
    "#function for decayed_learning_rate\n",
    "def decayed_lr(step):\n",
    "    return initial_learning_rate * decay_rate ^ (steps/ decay_steps)\n",
    "\n",
    "#if staircase argument is true then the lr follows a staircase pattern of decay\n",
    "\n",
    "#so basically this is an algorithm that takes the steps an an arguments and gives a decayed laerning rate as an output.\n",
    "\n",
    "#example\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=0.1,\n",
    "    decay_steps=100000,\n",
    "    decay_rate=0.96,\n",
    "    staircase=True)\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=lr_schedule),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(data, labels, epochs=5)\n",
    "\n",
    "#this learning rate schedule is serializable and deserializable using the tf.keras.optimizers.schedules.serialize and tf.keras.optimizers.schedules.deserialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-7e4041ff47dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#piecewise constant decay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschedules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPiecewiseConstantDecay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboundaries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#The function returns a 1-arg callable to compute the piecewise constant when passed the current optimizer step. This can be useful for changing the learning rate value across different invocations of optimizer functions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "#piecewise constant decay\n",
    "\n",
    "tf.keras.optimizers.schedules.PiecewiseConstantDecay(boundaries, values, name)\n",
    "\n",
    "#The function returns a 1-arg callable to compute the piecewise constant when passed the current optimizer step. This can be useful for changing the learning rate value across different invocations of optimizer functions.\n",
    "#this function enables us to use different learning rates for different steps\n",
    "\n",
    "step = tf.Variable(0, trainable=False)\n",
    "boundaries = [100000, 110000]\n",
    "values = [1.0, 0.5, 0.1]\n",
    "learning_rate_fn = keras.optimizers.schedules.PiecewiseConstantDecay(\n",
    "    boundaries, values)\n",
    "\n",
    "# Later, whenever we perform an optimization step, we pass in the step.\n",
    "learning_rate = learning_rate_fn(step)\n",
    "\n",
    "#Returns\n",
    "\"\"\"\n",
    "A 1-arg callable learning rate schedule that takes the current optimizer step and outputs the decayed learning rate, a scalar Tensor of the same type as the boundary tensors.\n",
    "\n",
    "The output of the 1-arg function that takes the step is values[0] when step <= boundaries[0], values[1] when step > boundaries[0] and step <= boundaries[1], ..., and values[-1] when step > boundaries[-1].\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-ce31b727dc12>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#polynomialdecay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschedules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPolynomialDecay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_learning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecay_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_learning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpower\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#this function schdeules a decay based on the initial llearning rate and the end learning rate passed as arguments.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "#polynomialdecay\n",
    "\n",
    "tf.keras.optimizers.schedules.PolynomialDecay(initial_learning_rate, decay_steps, end_learning_rate, power=1.0, cycle=False, name=None)\n",
    "\n",
    "#this function schdeules a decay based on the initial llearning rate and the end learning rate passed as arguments.\n",
    "#It requires a step value to compute the decayed learning rate. You can just pass a TensorFlow variable that you increment at each training step.\n",
    "\n",
    "def decayed_lr(step):\n",
    "    step= min(step, decay_steps)\n",
    "    return ((initial_learning_rate-end_learning_rate)*(1-step/ decay_steps)^(power))+ end_learning_rate\n",
    "\n",
    "#with cyles=true\n",
    "\n",
    "def decayed_lr(step):\n",
    "    decay_steps= decay_steps * ceil(step/ decay_steps)\n",
    "    return ((initial_learning_rate-end_learning_rate)*(1-step/ decay_steps)^(power))+ end_learning_rate\n",
    "\n",
    "\n",
    "#example\n",
    "\n",
    "starter_learning_rate = 0.1\n",
    "end_learning_rate = 0.01\n",
    "decay_steps = 10000\n",
    "learning_rate_fn = tf.keras.optimizers.schedules.PolynomialDecay(\n",
    "    starter_learning_rate,\n",
    "    decay_steps,\n",
    "    end_learning_rate,\n",
    "    power=0.5)\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.SGD(\n",
    "                  learning_rate=learning_rate_fn),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(data, labels, epochs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-9e5e6ba5319a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#invtime decay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m tf.keras.optimizers.schedules.InverseTimeDecay(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0minitial_learning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecay_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecay_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstaircase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m )\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "#invtime decay\n",
    "tf.keras.optimizers.schedules.InverseTimeDecay(\n",
    "    initial_learning_rate, decay_steps, decay_rate, staircase=False, name=None\n",
    ")\n",
    "\n",
    "#When training a model, it is often recommended to lower the learning rate as the training progresses. This schedule applies the inverse decay function to an optimizer step, given a provided initial learning rate. It requires a step value to compute the decayed learning rate. You can just pass a TensorFlow variable that you increment at each training step.\n",
    "\n",
    "def decayed_learning_rate(step):\n",
    "    \n",
    "    return initial_learning_rate/ (1+ decay_rate * step/decay_step)\n",
    "\n",
    "#if stircase is true\n",
    "\n",
    "def decayed_learning_rate(step):\n",
    "    \n",
    "    return initial_learning_rate/ (1+ decay_rate * floor(step/decay_step))\n",
    "\n",
    "#examples\n",
    "\n",
    "initial_learning_rate = 0.1\n",
    "decay_steps = 1.0\n",
    "decay_rate = 0.5\n",
    "learning_rate_fn = keras.optimizers.schedules.InverseTimeDecay(\n",
    "  initial_learning_rate, decay_steps, decay_rate)\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.SGD(\n",
    "                  learning_rate=learning_rate_fn),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(data, labels, epochs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
