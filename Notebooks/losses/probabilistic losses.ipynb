{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probabilistic losses\n",
    "\n",
    "# BinaryCrossentropy class\n",
    "\n",
    "syntax: tf.keras.losses.BinaryCrossentropy(from_logits=False, label_smoothing=0, reduction=\"auto\", name='binary_crossentropy')\n",
    "    \n",
    "1. computes crossentropy losses between true labels and predicted labels.\n",
    "\n",
    "2. Use this cross-entropy loss when there are only two label classes (assumed to be 0 and 1). For each example, there should be a single floating-point value per prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_tr and y_pred have shape [batch_size]\n",
    "import tensorflow as tf\n",
    "\n",
    "y_tr = [[0.,1.],[0.,0.]]\n",
    "y_pred = [[0.6,0.4],[0.4,0.6]]\n",
    "loss = tf.keras.losses.BinaryCrossentropy(reduction='sum')\n",
    "\n",
    "o1 = loss(y_tr, y_pred).numpy()\n",
    "\n",
    "\n",
    "o2 = loss(y_tr, y_pred, sample_weight=[1,0]).numpy()\n",
    "\n",
    "\n",
    "o3 = loss(y_tr, y_pred).numpy() #for reduction=\"sum\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CategoricalCrossentropy class\n",
    "\n",
    "syntax: tf.keras.losses.CategoricalCrossentropy(from_logits=False,label_smoothing=0,reduction=\"auto\",name=\"categorical_crossentropy\")\n",
    "\n",
    "1. Computes the crossentropy loss between the labels and predictions.\n",
    "\n",
    "2. Use this crossentropy loss function when there are two or more label classes. We expect labels to be provided in a one_hot representation. If you want to provide labels as integers, please use SparseCategoricalCrossentropy loss. There should be # classes floating point values per feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #y_tr and y_pred have shape [batch_size,num_classes]\n",
    "import tensorflow as tf\n",
    "\n",
    "y_tr = [[0,1,0],[0,0,1]]\n",
    "y_pred = [[0.6,0.4,0.06],[0.4,0.6,0.132]]\n",
    "loss = tf.keras.losses.CategoricalCrossentropy(reduction=\"sum_over_batch_size\")\n",
    "\n",
    "o1 = loss(y_tr, y_pred).numpy()\n",
    "\n",
    "\n",
    "o2 = loss(y_tr, y_pred, sample_weight=[1,0]).numpy()\n",
    "\n",
    "\n",
    "o3 = loss(y_tr, y_pred).numpy() #for reduction=\"sum\"\n",
    "\n",
    "o4 = loss(y_tr, y_pred).numpy() #for reduction=\"sum_over_batch_size\"\n",
    "\n",
    "o4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SparseCategoricalCrossentropy\n",
    "\n",
    "syntax: tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False, reduction=\"auto\", name=\"sparse_categorical_crossentropy\")\n",
    "    \n",
    "1. Computes the crossentropy loss between the labels and predictions.\n",
    "\n",
    "2. Use this crossentropy loss function when there are two or more label classes. We expect labels to be provided as integers. If you want to provide labels using one-hot representation, please use CategoricalCrossentropy loss. There should be #classes floating point values per feature for y_pred and a single floating point value per feature for y_true.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The shape of y_true is [batch_size] and the shape of y_pred is [batch_size, num_classes].\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "y_tr = [1,2]\n",
    "\n",
    "y_pred = [[0.05, 0.95, 0], [0.1,0.8,0.1]]\n",
    "\n",
    "scce = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "scce(y_tr, y_pred).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Poisson\n",
    "\n",
    "syntax: tf.keras.losses.Poisson(reduction=\"auto\", name=\"poisson\")\n",
    "\n",
    "1. Computes the poisson loss between y_tr and y_pred\n",
    "\n",
    "2. the formula is given by \n",
    "\n",
    "loss= y_pred - y_tr * log(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normally, floating values are passed in the input\n",
    "import tensorflow as tf\n",
    "\n",
    "y_tr = [[0.,1.],[0.,0.]]\n",
    "\n",
    "y_pred = [[1.,1.],[0.,0.]]\n",
    "\n",
    "p = tf.keras.losses.Poisson(reduction=\"sum\")\n",
    "\n",
    "o1 = p(y_tr, y_pred).numpy()\n",
    "\n",
    "o2 = p(y_tr, y_pred).numpy()\n",
    "\n",
    "o2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# binary_crossentropy function\n",
    "\n",
    "syntax: tf.keras.losses.binary_crossentropy(y_true, y_pred, from_logits=False, label_smoothing=0)\n",
    "\n",
    "1. Computes the binary crossentropy loss.\n",
    "\n",
    "\n",
    "# Returns\n",
    "\n",
    "Binary crossentropy loss value. shape = [batch_size, d0, .. dN-1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "y_true = [[0,1],[0,0]]\n",
    "\n",
    "y_pred = [[0.6,0.4],[0.4,0.6]]\n",
    "\n",
    "loss= tf.keras.losses.binary_crossentropy(y_true,y_pred)\n",
    "\n",
    "assert loss.shape == (2,)\n",
    "loss.numpy().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# categorical_crossentropy function\n",
    "\n",
    "syntax: tf.keras.losses.categorical_crossentropy(y_true, y_pred, from_logits=False, label_smoothing=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "y_true = [[0,1,0],[0,0,1]]\n",
    "\n",
    "y_pred = [[0.05,0.95,0],[0.1,0.8,0.1]]\n",
    "\n",
    "loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred)\n",
    "\n",
    "assert loss.shape == (2,)\n",
    "\n",
    "loss.numpy().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sparse_categorical_crossentropy function\n",
    "\n",
    "syntax: tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred, from_logits=False, axis=-1)\n",
    "\n",
    "1. Computes the sparse categorical crossentropy loss.\n",
    "\n",
    "# Returns\n",
    "\n",
    "Sparse categorical crossentropy loss value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "y_true = [1,2]\n",
    "\n",
    "y_pred = [[0.05,0.95,0],[0.1,0.8,0.1]]\n",
    "\n",
    "loss = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred)\n",
    "\n",
    "assert loss.shape == (2,)\n",
    "\n",
    "loss.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# poisson function\n",
    "\n",
    "sytax: tf.keras.losses.poisson(y_true, y_pred)\n",
    "\n",
    "1. Computes the Poisson loss between y_true and y_pred.\n",
    "\n",
    "2. The Poisson loss is the mean of the elements of the Tensor \n",
    "\n",
    "y_pred - y_true * log(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "y_true = np.random.randint(0,2,size=(2,3))\n",
    "\n",
    "y_pred = np.random.random(size=(2,3))\n",
    "\n",
    "loss = tf.keras.losses.poisson(y_true, y_pred)\n",
    "\n",
    "o1 = loss.numpy()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "assert loss.shape == (2,)\n",
    "\n",
    "y_pred = y_pred+1e-7\n",
    "\n",
    "assert np.allclose(loss.numpy(), np.mean(y_pred - y_true * np.log(y_pred), axis=-1), atol=1e-5)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "o1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KLDivergence class\n",
    "\n",
    "syntax: tf.keras.losses.KLDivergence(reduction=\"auto\",name=\"kl_divergence\")\n",
    "\n",
    "1. Computes Kullback-Leibler divergence loss between y_true and y_pred.\n",
    "\n",
    " loss = y_true * log(y_true / y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [[0,1],[0,0]]\n",
    "\n",
    "y_pred = [[0.6,0.4],[0.4,0.6]]\n",
    "\n",
    "kl = tf.keras.losses.KLDivergence()\n",
    "\n",
    "o1 = kl(y_true, y_pred).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kl_divergence function\n",
    "\n",
    "syntax : tf.keras.losses.kl_divergence(y_true, y_pred)\n",
    "\n",
    "1. Computes Kullback-Leibler divergence loss between y_true and y_pred.\n",
    "\n",
    "loss = y_true * log(y_true / y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np\n",
    "\n",
    "y_true = np.random.randint(0,2,size=(2,3)).astype(np.float64)\n",
    "\n",
    "y_pred = np.random.random(size=(2,3))\n",
    "\n",
    "loss = tf.keras.losses.kullback_leibler_divergence(y_true, y_pred)\n",
    "\n",
    "o1 = loss.numpy()\n",
    "\n",
    "o1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
