{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 0.0, 0.0]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ReLU \n",
    "\n",
    "#method \n",
    "#tf.keras.layers.ReLU(max_value=None, negative_slope=0, threshold=0)\n",
    "\n",
    "#With default values, it returns element-wise max(x, 0).\n",
    "\n",
    "#it follows\n",
    "\n",
    "\"\"\"\n",
    "f(x) = max_value if x >= max_value\n",
    "  f(x) = x if threshold <= x < max_value\n",
    "  f(x) = negative_slope * (x - threshold) otherwise\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#examples\n",
    "import tensorflow as tf\n",
    "layer=tf.keras.layers.ReLU()\n",
    "out = layer([1.0,-2.0, -3.24])\n",
    "\n",
    "out.numpy().tolist()\n",
    "\n",
    "\"\"\"\n",
    "Input shape\n",
    "\n",
    "Arbitrary. Use the keyword argument input_shape (tuple of integers, does not include the batch axis) when using this layer as the first layer in a model.\n",
    "\n",
    "Output shape\n",
    "\n",
    "Same shape as the input.\n",
    "\n",
    "Arguments\n",
    "\n",
    "    max_value: Float >= 0. Maximum activation value. Default to None, which means unlimited.\n",
    "    negative_slope: Float >= 0. Negative slope coefficient. Default to 0.\n",
    "    threshold: Float. Threshold value for thresholded activation. Default to 0.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.03205860328008499,\n",
       " 0.08714431874203257,\n",
       " 0.23688281808991013,\n",
       " 0.6439142598879724]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#softmax\n",
    "import numpy as np\n",
    "\n",
    "#method\n",
    "tf.keras.layers.Softmax(axis=-1)\n",
    "\n",
    "\"\"\"\n",
    "Input shape\n",
    "\n",
    "Arbitrary. Use the keyword argument input_shape (tuple of integers, does not include the samples axis) when using this layer as the first layer in a model.\n",
    "\n",
    "Output shape\n",
    "\n",
    "Same shape as the input.\n",
    "\n",
    "Arguments\n",
    "\n",
    "    axis: Integer, or list of Integers, axis along which the softmax normalization is applied.\n",
    "\n",
    "Call arguments\n",
    "\n",
    "    inputs: The inputs, or logits to the softmax layer.\n",
    "    mask: A boolean mask of the same shape as inputs. Defaults to None.\n",
    "\n",
    "Returns\n",
    "\n",
    "softmaxed output with the same shape as inputs.\n",
    "\n",
    "\"\"\"\n",
    "#example\n",
    "\n",
    "input_shape = np.asarray([1.,2.,3.,4.])\n",
    "layer = tf.keras.layers.Softmax()\n",
    "out = layer(input_shape)\n",
    "\n",
    "out.numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.9000000357627869, -1.350000023841858, -0.30000001192092896]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#leakyReLU\n",
    "\n",
    "tf.keras.layers.LeakyReLU(alpha=0.3)\n",
    "\n",
    "#logic\n",
    "#f(x) = alpha * x if x < 0\n",
    "#f(x) = x if x >= 0\n",
    "\n",
    "\"\"\"\n",
    "nput shape\n",
    "\n",
    "Arbitrary. Use the keyword argument input_shape (tuple of integers, does not include the batch axis) when using this layer as the first layer in a model.\n",
    "\n",
    "Output shape\n",
    "\n",
    "Same shape as the input.\n",
    "\n",
    "Arguments\n",
    "\n",
    "alpha: Float >= 0. Negative slope coefficient. Default to 0.3.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "layer = tf.keras.layers.LeakyReLU()\n",
    "\n",
    "out = layer([-3.0,-4.5,-1.0])\n",
    "\n",
    "out.numpy().tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PReLU class\n",
    "\n",
    "tf.keras.layers.PReLU(\n",
    "    alpha_initializer=\"zeros\",\n",
    "    alpha_regularizer=None,\n",
    "    alpha_constraint=None,\n",
    "    shared_axes=None,\n",
    "    **kwargs\n",
    ")\n",
    "\n",
    "#Parametric Rectified Linear Unit.\n",
    "\n",
    "#It follows:\n",
    "\"\"\"\n",
    "  f(x) = alpha * x for x < 0\n",
    "  f(x) = x for x >= 0\n",
    "\"\"\"\n",
    "\n",
    "where alpha is a learned array with the same shape as x.\n",
    "\"\"\"\n",
    "Input shape\n",
    "\n",
    "Arbitrary. Use the keyword argument input_shape (tuple of integers, does not include the samples axis) when using this layer as the first layer in a model.\n",
    "\n",
    "Output shape\n",
    "\n",
    "Same shape as the input.\n",
    "\n",
    "Arguments\n",
    "\n",
    "    alpha_initializer: Initializer function for the weights.\n",
    "    alpha_regularizer: Regularizer for the weights.\n",
    "    alpha_constraint: Constraint for the weights.\n",
    "    shared_axes: The axes along which to share learnable parameters for the activation function. For example, if the incoming feature maps are from a 2D convolution with output shape (batch, height, width, channels), and you wish to share parameters across space so that each filter only has one set of parameters, set shared_axes=[1, 2].\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'kwargs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-d460069ca391>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#ELU class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mELU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \"\"\"\n\u001b[1;32m      5\u001b[0m \u001b[0mExponential\u001b[0m \u001b[0mLinear\u001b[0m \u001b[0mUnit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'kwargs' is not defined"
     ]
    }
   ],
   "source": [
    "#ELU class\n",
    "\n",
    "tf.keras.layers.ELU(alpha=1.0, **kwargs)\n",
    "\"\"\"\n",
    "Exponential Linear Unit.\n",
    "\n",
    "It follows:\n",
    "\n",
    "  f(x) =  alpha * (exp(x) - 1.) for x < 0\n",
    "  f(x) = x for x >= 0\n",
    "\n",
    "Input shape\n",
    "\n",
    "Arbitrary. Use the keyword argument input_shape (tuple of integers, does not include the samples axis) when using this layer as the first layer in a model.\n",
    "\n",
    "Output shape\n",
    "\n",
    "Same shape as the input.\n",
    "\n",
    "Arguments\n",
    "\n",
    "alpha: Scale for the negative factor.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#thresholdedReLU class\n",
    "\n",
    "tf.keras.layers.ThresholdedReLU(theta=1.0, **kwargs)\n",
    "\n",
    "\"\"\"\n",
    "Thresholded Rectified Linear Unit.\n",
    "\n",
    "It follows:\n",
    "\n",
    "  f(x) = x for x > theta\n",
    "  f(x) = 0 otherwise`\n",
    "\n",
    "Input shape\n",
    "\n",
    "Arbitrary. Use the keyword argument input_shape (tuple of integers, does not include the samples axis) when using this layer as the first layer in a model.\n",
    "\n",
    "Output shape\n",
    "\n",
    "Same shape as the input.\n",
    "\n",
    "Arguments\n",
    "\n",
    "theta: Float >= 0. Threshold location of activation.\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
