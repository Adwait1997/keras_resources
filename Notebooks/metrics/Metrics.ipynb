{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics\n",
    "\n",
    "Metric is a function that assesses the performance of the model. Unlike the loss function, metrics are not used while training a model, but every loss function can be passed as a metric. \n",
    "\n",
    "We have 6 variants of metrics available in keras\n",
    "\n",
    "1. Accuracy metrics\n",
    "\n",
    "2. Probabilistic metrics\n",
    "\n",
    "3. Regression metrics\n",
    "\n",
    "4. Classification metrics based on True/False positives & negatives\n",
    "\n",
    "5. Image segmentation metrics\n",
    "\n",
    "6. Hinge metrics  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usage with compile() and fit() methods\n",
    "\n",
    "\n",
    "1. Metrics can be passed as an argument in compile() method\n",
    "\n",
    "**model.compile(optimizer='adam',loss='mean_squared_error, metrics=[metrics.MeanSquaredError(), metrics.AUC])**\n",
    "\n",
    "\n",
    "**model.compile(optimizer='adam',loss='mean_squared_error, metrics=['MeanSquaredError','AUC'])**\n",
    "\n",
    "2. metrics are returnable objects when evaluate() and fit() f\n",
    "functions are executed.\n",
    "\n",
    "\n",
    "3. Unlike losses, metrics are stateful, i.e., one can reset and update their state values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy metrics\n",
    "\n",
    "syntax: tf.keras.metrics.Accuracy(name='accuracy', dtype=None)\n",
    "\n",
    "1. It's calculation is based on how often the prediction is equal to the labels.\n",
    "\n",
    "\n",
    "2. It has 2 variables total and count to compute the frequency with which y_pred and y_true match. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Example\n",
    "\n",
    "import tensorflow as tf\n",
    "acc = tf.keras.metrics.Accuracy()\n",
    "acc.update_state([[1], [3], [5], [7]], [[1], [3], [5], [6]])\n",
    "acc.result().numpy()\n",
    "\n",
    "acc.reset_states()\n",
    "\n",
    "acc.update_state([[1], [23], [5], [7]], [[1], [3], [5], [7]], sample_weight=[1,1,0,0])\n",
    "\n",
    "acc.result().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Accuracy\n",
    "\n",
    "syntax: tf.keras.metrics.BinaryAccuracy(name=\"binary_accuracy\", dtype=None, thershold=0.5)\n",
    "\n",
    "\n",
    "1. This calculates the number of times prediction matches the binary labels.\n",
    "\n",
    "# Arguments\n",
    "\n",
    "\n",
    "1.    name: (Optional) string name of the metric instance.\n",
    "\n",
    "2.    dtype: (Optional) data type of the metric result.\n",
    "\n",
    "3.    threshold: (Optional) Float representing the threshold for deciding whether prediction values are 1 or 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Example\n",
    "\n",
    "import tensorflow as tf\n",
    "acc = tf.keras.metrics.BinaryAccuracy()\n",
    "acc.update_state([[1], [1], [0], [0]], [[0.98], [1], [0], [0.6]])\n",
    "\n",
    "acc.reset_states()\n",
    "\n",
    "acc.update_state([[1], [1], [0], [0]], [[0.98], [1], [0], [0.6]], sample_weight=[1,1,1,0])\n",
    "\n",
    "acc.result().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical Accuracy\n",
    "\n",
    "syntax: tf.keras.metrics.CategoricalAccuracy(name=\"categorical_accuracy\", dtype=None)\n",
    "\n",
    "\n",
    "1. This calculates the number of times prediction matches one-hot labels.\n",
    "\n",
    "\n",
    "2. It is mainly used in multi-class classification.\n",
    "\n",
    "\n",
    "3. y_pred and y_true should be passed in as vectors of probabilities, rather than as labels. If necessary, use tf.one_hot to expand y_true as a vector.\n",
    "\n",
    "# Arguments\n",
    "\n",
    "\n",
    "1.    name: (Optional) string name of the metric instance.\n",
    "\n",
    "2.    dtype: (Optional) data type of the metric result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#Example\n",
    "\n",
    "import tensorflow as tf\n",
    "acc = tf.keras.metrics.CategoricalAccuracy()\n",
    "acc.update_state([[1], [1], [0], [0]], [[0.98], [1], [0], [0.6]])\n",
    "\n",
    "acc.reset_states()\n",
    "\n",
    "acc.update_state([[1], [1], [0], [0]], [[0.98], [1], [0], [0.6]], sample_weight=[1,1,1,0])\n",
    "\n",
    "acc.result().numpy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  TopK Categorical Accuracy\n",
    "\n",
    "syntax: tf.keras.metrics.TopKCategoricalAccuracy(k=5,name=\"top_k_categorical_accuracy\", dtype=None)\n",
    "\n",
    "\n",
    "1. This calculates the number of times prediction matches Top K  predictions..\n",
    "\n",
    "\n",
    "2. It is mainly used in multi-class classification.\n",
    "\n",
    "\n",
    "3. y_pred and y_true should be passed in as vectors of probabilities, rather than as labels. If necessary, use tf.one_hot to expand y_true as a vector.\n",
    "\n",
    "# Arguments\n",
    "\n",
    "\n",
    "1.    name: (Optional) string name of the metric instance.\n",
    "\n",
    "2.    dtype: (Optional) data type of the metric result.\n",
    "\n",
    "3.     k = (Optional) Number of top elements to look at for computing accuracy. Defaults to 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Example\n",
    "\n",
    "import tensorflow as tf\n",
    "acc = tf.keras.metrics.TopKCategoricalAccuracy()\n",
    "acc.update_state([[0,0,1], [0.1, 0.9, 0.8]], [[0.05, 0.95, 0], [0.5,0.43, 0.32]])\n",
    "acc.result().numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spearseTopK Categorical Accuracy\n",
    "\n",
    "syntax: tf.keras.metrics.SparseTopKCategoricalAccuracy(k=5, name=\"SparseTopKCategoricalAccuracy\", dtype=None)\n",
    "\n",
    "1. Computes how often integer targets are in the top K predictions.\n",
    "\n",
    "# Arguments\n",
    "\n",
    "1.k: (Optional) Number of top elements to look at for computing accuracy. Defaults to 5.\n",
    "    \n",
    "    \n",
    "2.name: (Optional) string name of the metric instance.\n",
    "   \n",
    "\n",
    "3.dtype: (Optional) data type of the metric result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Example\n",
    "\n",
    "import tensorflow as tf\n",
    "acc = tf.keras.metrics.TopKCategoricalAccuracy()\n",
    "acc.update_state([[0, 1, 1], [1,0,0]], [[0.1, 0.9, 0.8], [0.05, 0.95, 0]])\n",
    "acc.result().numpy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
