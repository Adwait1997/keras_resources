{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is ROC?\n",
    "\n",
    "Receiver Operating Characteristics curve or (ROC) curve is a graphical plot that illustrates the capability of a binary classifier to classify or discriminate\n",
    "\n",
    "\n",
    "The ROC curve is created by plotting the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings. The true-positive rate is also known as sensitivity, recall or probability of detection.\n",
    "\n",
    "\n",
    "The false-positive rate is also known as probability of false alarm and can be calculated as (1 − specificity). \n",
    "\n",
    "\n",
    "# Basic concept\n",
    "\n",
    "\n",
    "True Positive: When condition is actually positive and prediction is also positive.\n",
    "    \n",
    "\n",
    "False Positive: When condition is actually negative but prediction is positive. (Type1 error)\n",
    "    \n",
    "\n",
    "False Negative: When condition is actually positive but prediction is also negative.\n",
    "    \n",
    "\n",
    "True Negative: When condition is actually negative and prediction is negative.\n",
    "\n",
    "    \n",
    "# Formulas\n",
    "\n",
    "\n",
    "1. True positive rate (TPR), Recall, Sensitivity, probability of detection, Power = Σ True positive/Σ Condition positive\n",
    "\n",
    "\n",
    "2. False positive rate (FPR), Fall-out, probability of false alarm = Σ False positive/Σ Condition negative\n",
    "\n",
    "\n",
    "3. False negative rate (FNR), Miss rate = Σ False negative/Σ Condition positive\n",
    "\n",
    "\n",
    "4. Specificity (SPC), Selectivity, True negative rate (TNR) = Σ True negative/Σ Condition negative\n",
    "\n",
    "\n",
    "# Curves in ROC space\n",
    "\n",
    "\n",
    "# Note\n",
    "\n",
    "In binary classification, the class prediction for each instance \n",
    "\n",
    "is often made based on a continuous random variable X , which is a \n",
    "\n",
    "\"score\" computed for the instance (e.g. the estimated probability \n",
    "\n",
    "in logistic regression). Given a threshold parameter T, the \n",
    "\n",
    "instance is classified as \"positive\" if X>T, and \"negative\" \n",
    "\n",
    "otherwise. X follows a probability density f1(x) f1(x) if the \n",
    "\n",
    "instance actually belongs to class \"positive\", and f0(x) if \n",
    "\n",
    "otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Area Under The Curve (AUC)\n",
    "\n",
    "1. When using normalized units, the area under the curve (often referred to as simply the AUC) is equal to the probability that a classifier will rank a randomly chosen positive instance higher than a randomly chosen negative one (assuming 'positive' ranks higher than 'negative').\n",
    "\n",
    "\n",
    "2. This can be seen as follows: the area under the curve is given by (the integral boundaries are reversed as large T has a lower value on the x-axis) \n",
    "    \n",
    "    \n",
    "\n",
    "TPR(T) : T->y(x)\n",
    "\n",
    "\n",
    "FPR(T): T->x\n",
    "    \n",
    "\n",
    "A = Integration (limits x=0 to x=1)(TPR(FPR^-1(x)))dx =\n",
    "\n",
    "\n",
    "Integration (limits x=-inf to x=inf)(TPR(T)(FPR'(T))dT = \n",
    "\n",
    "\n",
    "Double Integration(limits x=-inf to x=inf)I(T'>T)f1(T')f0(T)dT'dT \n",
    "\n",
    "= P(X1>X0)\n",
    "\n",
    "where X 1 is the score for a positive instance and X 0 is the \n",
    "\n",
    "score for a negative instance, and f0 and f1 are probability \n",
    "\n",
    "densities as defined in previous section. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# classification metrics based on true/false positives\n",
    "\n",
    "# AUC Class\n",
    "\n",
    "syntax: tf.keras.metrics.AUC(num_thresholds=200, curve=\"ROC\", summation_method=\"interpolation\", name=None, dtype=None, thresholds=None, multi_label=False, label_weights=None)\n",
    "    \n",
    "# Arguments\n",
    "\n",
    "1. num_threshold: Defaults 200. The number of threshold to use when discretizing ROC curve. Value must be > 0.\n",
    "\n",
    "    \n",
    "\n",
    "2. curve: (Optional) Specifies the name of the curve to be computed, 'ROC' [default] or 'PR' for the Precision-Recall-curve.\n",
    "\n",
    "    \n",
    "    \n",
    "3. summation_method: (Optional) Specifies the Riemann summation method used. 'interpolation' (default) applies mid-point summation scheme for ROC. For PR-AUC, interpolates (true/false) positives but not the ratio that is precision (see Davis & Goadrich 2006 for details); 'minoring' applies left summation for increasing intervals and right summation for decreasing intervals; 'majoring' does the opposite.\n",
    "\n",
    "    \n",
    "    \n",
    "4. name: (Optional) string name of the metric instance.\n",
    "\n",
    "\n",
    "\n",
    "5. dtype: (Optional) data type of the metric result.\n",
    "\n",
    "\n",
    "    \n",
    "6. thresholds: (Optional) A list of floating point values to use as the thresholds for discretizing the curve. If set, the num_thresholds parameter is ignored. Values should be in [0, 1]. Endpoint thresholds equal to {-epsilon, 1+epsilon} for a small positive epsilon value will be automatically included with these to correctly handle predictions equal to exactly 0 or 1.\n",
    "\n",
    "    \n",
    "\n",
    "7. multi_label: boolean indicating whether multilabel data should be treated as such, wherein AUC is computed separately for each label and then averaged across labels, or (when False) if the data should be flattened into a single label before AUC computation. In the latter case, when multilabel data is passed to AUC, each label-prediction pair is treated as an individual data point. Should be set to False for multi-class data.\n",
    "\n",
    "    \n",
    "\n",
    "8. label_weights: (optional) list, array, or tensor of non-negative weights used to compute AUCs for multilabel data. When multi_label is True, the weights are applied to the individual label AUCs when they are averaged to produce the multi-label AUC. When itss False, they are used to weight the individual label predictions in computing the confusion matrix on the flattened data. Note that this is unlike class_weights in that class_weights weights the example depending on the value of its label, whereas label_weights depends only on the index of that label before flattening; therefore label_weights should not be used for multi-class data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examples\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "m= tf.keras.metrics.AUC(num_thresholds=3)\n",
    "\n",
    "m.update_state([0,0,1,1], [0,0.5,0.3,0.9])\n",
    "\n",
    "m.result().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precision\n",
    "\n",
    "syntax : tf.keras.metrics.Precision(thresholds=None, top_k=None, class_id=None, name=None, dtype=None)\n",
    "  \n",
    "\n",
    "1. Computes the precision of the predictions with respect to the labels.\n",
    "\n",
    "\n",
    "\n",
    "2. The metric creates two local variables, true_positives and false_positives that are used to compute the precision. This value is ultimately returned as precision, an idempotent operation that simply divides true_positives by the sum of true_positives and false_positives.\n",
    "\n",
    "\n",
    "\n",
    "3. If sample_weight is None, weights default to 1. Use sample_weight of 0 to mask values.\n",
    "\n",
    "\n",
    "4. If top_k is set, we'll calculate precision as how often on average a class among the top-k classes with the highest predicted values of a batch entry is correct and can be found in the label for that entry.\n",
    "\n",
    "\n",
    "5. If class_id is specified, we calculate precision by considering only the entries in the batch for which class_id is above the threshold and/or in the top-k highest predictions, and computing the fraction of them for which class_id is indeed a correct label.\n",
    "\n",
    "\n",
    "# Arguments\n",
    "\n",
    "\n",
    "1. thresholds: (Optional) A float value or a python list/tuple of float threshold values in [0, 1]. A threshold is compared with prediction values to determine the truth value of predictions (i.e., above the threshold is true, below is false). One metric value is generated for each threshold value. If neither thresholds nor top_k are set, the default is to calculate precision with thresholds=0.5.\n",
    "\n",
    "\n",
    "2. top_k: (Optional) Unset by default. An int value specifying the top-k predictions to consider when calculating precision.\n",
    "\n",
    "\n",
    "3. class_id: (Optional) Integer class ID for which we want binary metrics. This must be in the half-open interval [0, num_classes), where num_classes is the last dimension of predictions.\n",
    "    \n",
    "    \n",
    "4. name: (Optional) string name of the metric instance.\n",
    "    \n",
    "    \n",
    "5. dtype: (Optional) data type of the metric result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Examples\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "m= tf.keras.metrics.Precision(top_k=2)\n",
    "\n",
    "m.update_state([0,1,1,1], [1,1,0,1])\n",
    "\n",
    "m.result().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "m= tf.keras.metrics.Precision(top_k=4)\n",
    "\n",
    "m.update_state([0,1,1,1], [1,1,0,1])\n",
    "\n",
    "m.result().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recall class\n",
    "\n",
    "syntax: tf.keras.metrics.Recall(thresholds=None, top_k=None, class_id=None, name=None, dtype=None)\n",
    "\n",
    "1. Computes the recall of the predictions with respect to the labels.\n",
    "\n",
    "2. his metric creates two local variables, true_positives and false_negatives, that are used to compute the recall. This value is ultimately returned as recall, an idempotent operation that simply divides true_positives by the sum of true_positives and false_negatives.\n",
    "\n",
    "3. If sample_weight is None, weights default to 1. Use sample_weight of 0 to mask values.\n",
    "\n",
    "4. If top_k is set, recall will be computed as how often on average a class among the labels of a batch entry is in the top-k predictions.\n",
    "\n",
    "5. If class_id is specified, we calculate recall by considering only the entries in the batch for which class_id is in the label, and computing the fraction of them for which class_id is above the threshold and/or in the top-k predictions.\n",
    "\n",
    "\n",
    "# Arguments\n",
    "\n",
    "\n",
    "1. thresholds: (Optional) A float value or a python list/tuple of float threshold values in [0, 1]. A threshold is compared with prediction values to determine the truth value of predictions (i.e., above the threshold is true, below is false). One metric value is generated for each threshold value. If neither thresholds nor top_k are set, the default is to calculate recall with thresholds=0.5.\n",
    "\n",
    "\n",
    "2. top_k: (Optional) Unset by default. An int value specifying the top-k predictions to consider when calculating recall.\n",
    "    \n",
    "    \n",
    "3. class_id: (Optional) Integer class ID for which we want binary metrics. This must be in the half-open interval [0, num_classes), where num_classes is the last dimension of predictions.\n",
    "    \n",
    "    \n",
    "4. name: (Optional) string name of the metric instance.\n",
    "  \n",
    "  \n",
    "5. dtype: (Optional) data type of the metric result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33333334"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "m= tf.keras.metrics.Recall(top_k=2)\n",
    "\n",
    "m.update_state([0,1,1,1], [1,1,0,1])\n",
    "\n",
    "m.result().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TruePositives class\n",
    "\n",
    "syntax :tf.keras.metrics.TruePositives(thresholds=None, name=None, dtype=None)\n",
    "\n",
    "1. Calculates the number of true positives.\n",
    "\n",
    "\n",
    "2. If sample_weight is given, calculates the sum of the weights of true positives. This metric creates one local variable, true_positives that is used to keep track of the number of true positives.\n",
    "\n",
    "\n",
    "3. If sample_weight is None, weights default to 1. Use sample_weight of 0 to mask values.\n",
    "\n",
    "\n",
    "# Arguments\n",
    "\n",
    "    \n",
    "1. thresholds: (Optional) Defaults to 0.5. A float value or a python list/tuple of float threshold values in [0, 1]. A threshold is compared with prediction values to determine the truth value of predictions (i.e., above the threshold is true, below is false). One metric value is generated for each threshold value.\n",
    "\n",
    "    \n",
    "2. name: (Optional) string name of the metric instance.\n",
    "    \n",
    "\n",
    "3. dtype: (Optional) data type of the metric result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "m= tf.keras.metrics.TruePositives()\n",
    "\n",
    "m.update_state([0,1,1,1], [1,1,0,1])\n",
    "\n",
    "m.result().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TrueNegatives class\n",
    "\n",
    "syntax: tf.keras.metrics.TrueNegatives(thresholds=None, name=None, dtype=None)\n",
    "\n",
    "1. Calculates the number of true negatives.\n",
    "\n",
    "2. If sample_weight is given, calculates the sum of the weights of true negatives. This metric creates one local variable, accumulator that is used to keep track of the number of true negatives.\n",
    "\n",
    "3. If sample_weight is None, weights default to 1. Use sample_weight of 0 to mask values.\n",
    "\n",
    "# Arguments\n",
    "\n",
    "1. thresholds: (Optional) Defaults to 0.5. A float value or a python list/tuple of float threshold values in [0, 1]. A threshold is compared with prediction values to determine the truth value of predictions (i.e., above the threshold is true, below is false). One metric value is generated for each threshold value.\n",
    "    \n",
    "    \n",
    "2. name: (Optional) string name of the metric instance.\n",
    "    \n",
    "    \n",
    "3. dtype: (Optional) data type of the metric result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "m= tf.keras.metrics.TrueNegatives()\n",
    "\n",
    "m.update_state([0,1,1,1], [1,1,0,1])\n",
    "\n",
    "m.result().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FalsePositives class\n",
    "\n",
    "syntax: tf.keras.metrics.FalsePositives(thresholds=None, name=None, dtype=None)\n",
    "\n",
    "\n",
    "1. Calculates the number of false positives.\n",
    "\n",
    "\n",
    "2. If sample_weight is given, calculates the sum of the weights of false positives. This metric creates one local variable, accumulator that is used to keep track of the number of false positives.\n",
    "\n",
    "\n",
    "3. If sample_weight is None, weights default to 1. Use sample_weight of 0 to mask values.\n",
    "\n",
    "# Arguments\n",
    "\n",
    "    \n",
    "1. thresholds: (Optional) Defaults to 0.5. A float value or a python list/tuple of float threshold values in [0, 1]. A threshold is compared with prediction values to determine the truth value of predictions (i.e., above the threshold is true, below is false). One metric value is generated for each threshold value.\n",
    "    \n",
    "\n",
    "2. name: (Optional) string name of the metric instance.\n",
    "    \n",
    "\n",
    "3. dtype: (Optional) data type of the metric result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "m= tf.keras.metrics.FalsePositives()\n",
    "\n",
    "m.update_state([0,1,1,1], [1,1,0,1])\n",
    "\n",
    "m.result().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FalseNegatives class\n",
    "\n",
    "syntax: tf.keras.metrics.FalseNegatives(thresholds=None, name=None, dtype=None)\n",
    "\n",
    "1. Calculates the number of false negatives.\n",
    "\n",
    "\n",
    "2. If sample_weight is given, calculates the sum of the weights of false negatives. This metric creates one local variable, accumulator that is used to keep track of the number of false negatives.\n",
    "\n",
    "\n",
    "3. If sample_weight is None, weights default to 1. Use sample_weight of 0 to mask values.\n",
    "\n",
    "# Arguments\n",
    "\n",
    "1. thresholds: (Optional) Defaults to 0.5. A float value or a python list/tuple of float threshold values in [0, 1]. A threshold is compared with prediction values to determine the truth value of predictions (i.e., above the threshold is true, below is false). One metric value is generated for each threshold value.\n",
    "    \n",
    "    \n",
    "2. name: (Optional) string name of the metric instance.\n",
    "    \n",
    "    \n",
    "3. dtype: (Optional) data type of the metric result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "m= tf.keras.metrics.FalseNegatives()\n",
    "\n",
    "m.update_state([0,1,1,1], [0,0,0,1])\n",
    "\n",
    "m.result().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PrecisionAtRecall class\n",
    "\n",
    "syntax: tf.keras.metrics.PrecisionAtRecall(recall,num_thresholds=200,name=None, dtype=None)\n",
    "    \n",
    "\n",
    "1. Computes best precision where recall is >= specified value.\n",
    "\n",
    "\n",
    "2. This metric creates four local variables, true_positives, true_negatives, false_positives and false_negatives that are used to compute the precision at the given recall. The threshold for the given recall value is computed and used to evaluate the corresponding precision.\n",
    "\n",
    "\n",
    "3. If sample_weight is None, weights default to 1. Use sample_weight of 0 to mask values.\n",
    "\n",
    "\n",
    "# Arguments\n",
    "\n",
    "1. recall: A scalar value in range [0, 1].\n",
    "    \n",
    "    \n",
    "2. num_thresholds: (Optional) Defaults to 200. The number of thresholds to use for matching the given recall.\n",
    "    \n",
    "    \n",
    "3. name: (Optional) string name of the metric instance.\n",
    "    \n",
    "    \n",
    "4. dtype: (Optional) data type of the metric result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow.python.keras.api._v2.keras.metrics' has no attribute 'PrecisionAtRecall'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-c3a7a8ef3004>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPrecisionAtRecall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow.python.keras.api._v2.keras.metrics' has no attribute 'PrecisionAtRecall'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "m = tf.keras.metrics.PrecisionAtRecall(0.5)\n",
    "\n",
    "m.update_state([0,0,1,0,1], [0,0.3,0.8,0.3,0.8])\n",
    "\n",
    "m.result().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SensitivityAtSpecificity class\n",
    "\n",
    "\n",
    "syntax: tf.keras.metrics.SensitivityAtSpecificity(specificity, num_thresholds=200, name=None, dtype=None)\n",
    "\n",
    "\n",
    "1. Computes best sensitivity where specificity is >= specified value.\n",
    "\n",
    "the sensitivity at a given specificity.\n",
    "\n",
    "\n",
    "2.Sensitivity measures the proportion of actual positives that are correctly identified as such (tp / (tp + fn)). Specificity measures the proportion of actual negatives that are correctly identified as such (tn / (tn + fp)).\n",
    "\n",
    "\n",
    "3. This metric creates four local variables, true_positives, true_negatives, false_positives and false_negatives that are used to compute the sensitivity at the given specificity. The threshold for the given specificity value is computed and used to evaluate the corresponding sensitivity.\n",
    "\n",
    "\n",
    "4. If sample_weight is None, weights default to 1. Use sample_weight of 0 to mask values.\n",
    "\n",
    "\n",
    "\n",
    "# Arguments\n",
    "\n",
    "1. specificity: A scalar value in range [0, 1].\n",
    "    \n",
    "    \n",
    "2. num_thresholds: (Optional) Defaults to 200. The number of thresholds to use for matching the given specificity.\n",
    "    \n",
    "    \n",
    "3. name: (Optional) string name of the metric instance.\n",
    "    \n",
    "    \n",
    "4. dtype: (Optional) data type of the metric result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "m = tf.keras.metrics.SensitivityAtSpecificity(0.5)\n",
    "m.update_state([0, 0, 0, 1, 1], [0, 0.3, 0.8, 0.3, 0.8])\n",
    "m.result().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SpecificityAtSensitivity class\n",
    "\n",
    "syntax: tf.keras.metrics.SpecificityAtSensitivity(sensitivity, num_thresholds=200, name=None, dtype=None)\n",
    "\n",
    "1. Computes best specificity where sensitivity is >= specified value.\n",
    "\n",
    "\n",
    "2. Sensitivity measures the proportion of actual positives that are correctly identified as such (tp / (tp + fn)). Specificity measures the proportion of actual negatives that are correctly identified as such (tn / (tn + fp)).\n",
    "\n",
    "\n",
    "3.This metric creates four local variables, true_positives, true_negatives, false_positives and false_negatives that are used to compute the specificity at the given sensitivity. The threshold for the given sensitivity value is computed and used to evaluate the corresponding specificity.\n",
    "\n",
    "\n",
    "4. If sample_weight is None, weights default to 1. Use sample_weight of 0 to mask values.\n",
    "\n",
    "\n",
    "\n",
    "# Arguments\n",
    "\n",
    "1. sensitivity: A scalar value in range [0, 1].\n",
    "    \n",
    "    \n",
    "2. num_thresholds: (Optional) Defaults to 200. The number of thresholds to use for matching the given sensitivity.\n",
    "    \n",
    "    \n",
    "3. name: (Optional) string name of the metric instance.\n",
    "    \n",
    "    \n",
    "4. dtype: (Optional) data type of the metric result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666667"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "m = tf.keras.metrics.SpecificityAtSensitivity(0.5)\n",
    "m.update_state([0, 0, 0, 1, 1], [0, 0.3, 0.8, 0.3, 0.8])\n",
    "m.result().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
