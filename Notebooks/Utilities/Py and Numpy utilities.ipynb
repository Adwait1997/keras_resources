{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# to_categorical function\n",
    "\n",
    "syntax: tf.keras.utils.to_categorical(y, num_classes=None, dtype=\"float32\")\n",
    "\n",
    "1. converts class vector into binary class matrix\n",
    "\n",
    "# arguments\n",
    "\n",
    "1. y: class vector to be converted into a matrix (integers from 0 to num_classes).\n",
    "\n",
    "    \n",
    "2. num_classes: total number of classes. If None, this would be inferred as the (largest number in y) + 1.\n",
    "    \n",
    "    \n",
    "3. dtype: The data type expected by the input. Default: 'float32'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=0, shape=(5, 5), dtype=float32, numpy=\n",
       "array([[1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "a = tf.keras.utils.to_categorical([0,1,2,3,4], num_classes=5)\n",
    "\n",
    "a = tf.constant(a, shape=[5,5])\n",
    "\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# normalize function\n",
    "\n",
    "syntax:tf.keras.utils.normalize(x, axis=-1, order=2)\n",
    "\n",
    "1. Normalizes a Numpy array.\n",
    "\n",
    "# Arguments\n",
    "\n",
    "1. x: Numpy array to normalize.\n",
    "    \n",
    "2. axis: axis along which to normalize.\n",
    "\n",
    "3. order: Normalization order (e.g. order=2 for L2 norm).\n",
    "\n",
    "# Returns\n",
    "\n",
    "1. A normalized copy of the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.70710678, 0.42426407, 0.56568542]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "a = tf.keras.utils.normalize([0.5,0.3,0.4], axis=0, order=2)\n",
    "\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get_file function\n",
    "\n",
    "syntax: tf.keras.utils.get_file(fname,origin,untar=False,md5_hash=None,file_hash=None,cache_subdir=\"datasets\",hash_algorithm=\"auto\",extract=False,archive_format=\"auto\",cache_dir=None,)\n",
    "\n",
    "1. Downloads a file from a URL if it not already in the cache.\n",
    "\n",
    "\n",
    "2. By default the file at the url origin is downloaded to the cache_dir ~/.keras, placed in the cache_subdir datasets, and given the filename fname. The final location of a file example.txt would therefore be ~/.keras/datasets/example.txt.\n",
    "\n",
    "# Arguments\n",
    "\n",
    "\n",
    "\n",
    "1. fname: Name of the file. If an absolute path /path/to/file.txt is specified the file will be saved at that location.\n",
    "    \n",
    "\n",
    "2. origin: Original URL of the file.\n",
    "    \n",
    "\n",
    "3. untar: Deprecated in favor of extract argument. boolean, whether the file should be decompressed\n",
    "    \n",
    "\n",
    "4. md5_hash: Deprecated in favor of file_hash argument. md5 hash of the file for verification\n",
    "    \n",
    "\n",
    "5. file_hash: The expected hash string of the file after download. The sha256 and md5 hash algorithms are both supported.\n",
    "    \n",
    "\n",
    "6. cache_subdir: Subdirectory under the Keras cache dir where the file is saved. If an absolute path /path/to/folder is specified the file will be saved at that location.\n",
    "    \n",
    "\n",
    "7. hash_algorithm: Select the hash algorithm to verify the file. options are 'md5', 'sha256', and 'auto'. The default 'auto' detects the hash algorithm in use.\n",
    "    \n",
    "\n",
    "8. extract: True tries extracting the file as an Archive, like tar or zip.\n",
    "    \n",
    "\n",
    "9. archive_format: Archive format to try for extracting the file. Options are 'auto', 'tar', 'zip', and None. 'tar' includes tar, tar.gz, and tar.bz files. The default 'auto' corresponds to ['tar', 'zip']. None or an empty list will return no matches found.\n",
    "    \n",
    "\n",
    "10. cache_dir: Location to store cached files, when None it defaults to the default directory ~/.keras/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Progbar class\n",
    "\n",
    "syntax: tf.keras.utils.Progbar(target, width=30, verbose=1, interval=0.05, stateful_metrics=None, unit_name=\"step\")\n",
    "\n",
    "1. Displays a progress bar.\n",
    "\n",
    "# Arguments\n",
    "\n",
    "\n",
    "1. target: Total number of steps expected, None if unknown.\n",
    "    \n",
    "\n",
    "2. width: Progress bar width on screen.\n",
    "    \n",
    "\n",
    "3. verbose: Verbosity mode, 0 (silent), 1 (verbose), 2 (semi-verbose)\n",
    "    \n",
    "\n",
    "4. stateful_metrics: Iterable of string names of metrics that should not be averaged over time. Metrics in this list will be displayed as-is. All others will be averaged by the progbar before display.\n",
    "    \n",
    "\n",
    "5. interval: Minimum visual progress update interval (in seconds).\n",
    "    \n",
    "\n",
    "6. unit_name: Display name for step counts (usually \"step\" or \"sample\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence class\n",
    "\n",
    "syntax: tf.keras.utils.Sequence()\n",
    "    \n",
    "1. Base object for fitting to a sequence of data, such as a dataset.\n",
    "\n",
    "2. Every Sequence must implement the __getitem__ and the __len__ methods. If you want to modify your dataset between epochs you may implement on_epoch_end. The method __getitem__ should return a complete batch.\n",
    "\n",
    "Notes:\n",
    "\n",
    "1. Sequence are a safer way to do multiprocessing. This structure guarantees that the network will only train once on each sample per epoch which is not the case with generators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__getitem__() missing 1 required positional argument: 'idx'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-5e4f1493f645>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__len__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: __getitem__() missing 1 required positional argument: 'idx'"
     ]
    }
   ],
   "source": [
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "import numpy as np\n",
    "import math\n",
    "# Here, `x_set` is list of path to the images\n",
    "# and `y_set` are the associated classes.\n",
    "\n",
    "class CIFAR10Sequence(tf.keras.utils.Sequence):\n",
    "\n",
    "    def __init__(self, x_set, y_set, batch_size):\n",
    "        self.x, self.y = x_set, y_set\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return math.ceil(len(self.x) / self.batch_size)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.x[idx * self.batch_size:(idx + 1) *\n",
    "        self.batch_size]\n",
    "        batch_y = self.y[idx * self.batch_size:(idx + 1) *\n",
    "        self.batch_size]\n",
    "\n",
    "        return np.array([\n",
    "            resize(imread(file_name), (200, 200))\n",
    "               for file_name in batch_x]), np.array(batch_y)\n",
    "\n",
    "dataset = CIFAR10Sequence(10,10,32)\n",
    "\n",
    "dataset.__len__\n",
    "\n",
    "dataset.__getitem__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
