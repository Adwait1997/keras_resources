{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Activations\n",
    "#Activation is a layer \n",
    "#it can be used as an argument inside the layers\n",
    "\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "#import numpy as np\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "\n",
    "#model.add(tf.keras.layers.Dense(10, activation=tf.keras.activations.relu))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(10, activation='relu')) #activation as argument\n",
    "model.add(tf.keras.layers.Activation(activation='relu')) #using the activation as an independent layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0., -0., -4., -2.], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#RELU\n",
    "#rectified linear unit activn fn\n",
    "#logic max(x,0) where X is the input tensor\n",
    "#This function returns the maximum among the input tensor or 0 when applied.\n",
    "#it is used to maintain the value above a certain threshold\n",
    "\n",
    "\n",
    "#tf.keras.activations.relu(x, alpha, threshold, max_value)\n",
    "\n",
    "#x is the input [tensor]\n",
    "#alpha controls the slope [float]\n",
    "#max_value sets the largest value the function can return [float]\n",
    "#threshold sets the threshold value below which all the values are zero or damped[float]\n",
    "\n",
    "\n",
    "f = tf.constant([-10, -5, -4, -2], dtype=tf.float32)\n",
    "out = tf.keras.activations.relu(f)\n",
    "out.numpy()\n",
    "\n",
    "out1 = tf.keras.activations.relu(f, threshold=-5)\n",
    "out1.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.0611537e-09, 4.5397872e-05, 5.0000000e-01, 7.3105860e-01,\n",
       "       8.8079703e-01], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sigmoid function\n",
    "#tf.keras.activations.sigmoid(x)\n",
    "\n",
    "#sigmoid(x) = 1 / (1 + exp(-x)).\n",
    "#for values(<-5) sigmoid returns and for values(>5) sigmoid returns values close to 1.\n",
    "#sigmoid always returns a value between\n",
    "\n",
    "a = tf.constant([-20.0,-10.0,0.0,1.0,2.0], dtype=tf.float32)\n",
    "b = tf.keras.activations.sigmoid(a)\n",
    "b.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#softmax \n",
    "#converts real vectors to the veactors of the categoricala probabilities \n",
    "#each output is in the range of (0,1)\n",
    "#softmax is often used as the activation for the last layer of a classification network.\n",
    "\n",
    "#tf.keras.activations.softmax(x, axis=-1)\n",
    "\n",
    "#example \n",
    "import tensorflow as tf\n",
    "from math import exp\n",
    "out = exp(2)/ tf.reduce_sum(exp(2))\n",
    "\n",
    "out.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.5417706e-05, 2.1269281e+00, 3.0485873e+00], dtype=float32)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#softplus\n",
    "#tf.keras.activations.softplus(x)\n",
    "#softplus(x) = log(exp(x)+1)\n",
    "\n",
    "\n",
    "x = tf.constant([-10.0,2.0,3.0], dtype=tf.float32)\n",
    "out = tf.keras.activations.softplus(x)\n",
    "out.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=154, shape=(5,), dtype=float32, numpy=\n",
       "array([-0.95238096, -0.90909094, -0.85714287,  0.8333333 ,  0.8       ],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#softsign function\n",
    "#tf.keras.activations.softsign(x)\n",
    "\n",
    "#softsign = x/(abs(x)+1)\n",
    "\n",
    "x = tf.constant([-20.0,-10.0,-6.0,5.0,4.0], dtype=tf.float32)\n",
    "out = tf.keras.activations.softsign(x)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.        , -1.        , -0.99998784,  0.99990916,  0.9993292 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tanh function\n",
    "#tf.keras.activations.tanh(x)\n",
    "#it is a hyperbolic tangent function\n",
    "\n",
    "#sinh(x)/ cosh(x) = exp(x)-exp(-x)/exp(x)+exp(-x)\n",
    "\n",
    "x = tf.constant([-20.0,-10.0,-6.0,5.0,4.0], dtype=tf.float32)\n",
    "out = tf.keras.activations.tanh(x)\n",
    "out.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nif x > 0: it returns sclae*x\\nif x < 0: it returns scale*alpha*(exp(x)-1)\\n\\nwhere alpha = 1.67326324 and scale = 1.05070098\\n\\n'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#selu function\n",
    "#tf.keras.activations.selu(x)\n",
    "#scaled exponential linear unit\n",
    "#logic \n",
    "\"\"\"\n",
    "if x > 0: it returns sclae*x\n",
    "if x < 0: it returns scale*alpha*(exp(x)-1)\n",
    "\n",
    "where alpha = 1.67326324 and scale = 1.05070098\n",
    "\n",
    "\"\"\"\n",
    "#SELU multiplies scale with the output of the ELU to ensure the slope is larger than one of the positive inputs\n",
    "\n",
    "#The scaled exponential unit activation: scale * elu(x, alpha).\n",
    "\n",
    "#Notes: - To be used together with the tf.keras.initializers.LecunNormal initializer. - To be used together with the dropout variant tf.keras.layers.AlphaDropout (not regular dropout)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nif x > 0 and alpha > 0: x\\nif x < 0 then x:alpha * (exp(x)-1)\\n\\nwhere alpha is a hyperparameter that controls that values used by alpha for saturating the negative inputs\\n'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#elu \n",
    "#exponential linear unit\n",
    "\"\"\"\n",
    "if x > 0 and alpha > 0: x\n",
    "if x < 0 then x:alpha * (exp(x)-1)\n",
    "\n",
    "where alpha is a hyperparameter that controls that values used by alpha for saturating the negative inputs\n",
    "\"\"\"\n",
    "\n",
    "#ELU solves the vanishing gradient problem\n",
    "#ELU reduces the variation by bringing the gradient closer to the natural gradient\n",
    "#when the variation is diminished, the learning is faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.0611537e-09, 4.5399931e-05, 2.4787523e-03, 1.4841316e+02,\n",
       "       5.4598148e+01], dtype=float32)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#exponential function\n",
    "#tf.keras.activations.exponential(x)\n",
    "\n",
    "x=a\n",
    "b = tf.keras.activations.exponential(a)\n",
    "b.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
