{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow.python.keras.api._v2.keras.layers' has no attribute 'Attention'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-bbff49b6fcf7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#this is a dot-product attention layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow.python.keras.api._v2.keras.layers' has no attribute 'Attention'"
     ]
    }
   ],
   "source": [
    "#attention layer\n",
    "#method \n",
    "import tensorflow as tf\n",
    "tf.keras.layers.Attention(use_scale=False)\n",
    "\n",
    "#this is a dot-product attention layer\n",
    "\n",
    "\"\"\"\n",
    "inputs are:\n",
    "\n",
    "query tensor of shape - [batch_size, Tq, dim]\n",
    "\n",
    "value tensor of shape - [batch_size, Tv, dim]\n",
    "\n",
    "key tensor of shape  - [batch_size, Tv, dim]\n",
    "\n",
    "Calculation:\n",
    "\n",
    "1. Calc scores with shape [batch_size, Tq, Tv] as query-key dot prod:\n",
    "\n",
    "scores = tf.matmul(query, key, transpose_b = True)\n",
    "\n",
    "2. Use scores to calc a distribution with shape [batch_size, Tq, Tv]\n",
    "\n",
    "distribution = tf.nn.softmax(scores)\n",
    "\n",
    "3. Use distribution to create a linear combination of value with shape [batch_size, Tq, dim]: return tf.matmul(distribution, value)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#arguments\n",
    "\n",
    "\"\"\"\n",
    "use_scale : [boolean], if true, it creates scalar variable to scale the attention scores.\n",
    "\n",
    "causal: [boolean] for decoding self-attention. \n",
    "\n",
    "dropout: [float 0 to 1] for elimination units for attention score\n",
    "\n",
    "return_attention_scores [boolean] if true, returns attention score after masking and softmax.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "#Inputs\n",
    "\"\"\"\n",
    "List of the following tensors: * query: Query Tensor of shape [batch_size, Tq, dim]. \n",
    "\n",
    "* value: Value Tensor of shape [batch_size, Tv, dim]. \n",
    "\n",
    "* key: Optional key Tensor of shape [batch_size, Tv, dim]. If not given, will use value for both key and value, which is the most common case. \n",
    "\n",
    "mask: List of the following tensors: \n",
    "\n",
    "* query_mask: A boolean mask Tensor of shape [batch_size, Tq]. If given, the output will be zero at the positions where mask==False. \n",
    "\n",
    "* value_mask: A boolean mask Tensor of shape [batch_size, Tv]. If given, will apply the mask such that values at positions where mask==False do not contribute to the result. \n",
    "\n",
    "training: Python boolean indicating whether the layer should behave in training mode (adding dropout) or in inference mode (no dropout).\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#Output\n",
    "\n",
    "\"\"\"\n",
    "Attention outputs of shape: [batch_size, Tq, dim]\n",
    "\n",
    "Attention scores after masking and softmax: [batch_size, Tq, Tv]\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Variable-length int sequences.\n",
    "query_input = tf.keras.Input(shape=(None,), dtype='int32')\n",
    "value_input = tf.keras.Input(shape=(None,), dtype='int32')\n",
    "\n",
    "# Embedding lookup.\n",
    "token_embedding = tf.keras.layers.Embedding(input_dim=1000, output_dim=64)\n",
    "# Query embeddings of shape [batch_size, Tq, dimension].\n",
    "query_embeddings = token_embedding(query_input)\n",
    "# Value embeddings of shape [batch_size, Tv, dimension].\n",
    "value_embeddings = token_embedding(value_input)\n",
    "\n",
    "# CNN layer.\n",
    "cnn_layer = tf.keras.layers.Conv1D(\n",
    "    filters=100,\n",
    "    kernel_size=4,\n",
    "    # Use 'same' padding so outputs have the same shape as inputs.\n",
    "    padding='same')\n",
    "# Query encoding of shape [batch_size, Tq, filters].\n",
    "query_seq_encoding = cnn_layer(query_embeddings)\n",
    "# Value encoding of shape [batch_size, Tv, filters].\n",
    "value_seq_encoding = cnn_layer(value_embeddings)\n",
    "\n",
    "# Query-value attention of shape [batch_size, Tq, filters].\n",
    "query_value_attention_seq = tf.keras.layers.Attention()(\n",
    "    [query_seq_encoding, value_seq_encoding])\n",
    "\n",
    "# Reduce over the sequence axis to produce encodings of shape\n",
    "# [batch_size, filters].\n",
    "query_encoding = tf.keras.layers.GlobalAveragePooling1D()(\n",
    "    query_seq_encoding)\n",
    "query_value_attention = tf.keras.layers.GlobalAveragePooling1D()(\n",
    "    query_value_attention_seq)\n",
    "\n",
    "# Concatenate query and document encodings to produce a DNN input layer.\n",
    "input_layer = tf.keras.layers.Concatenate()(\n",
    "    [query_encoding, query_value_attention])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow.python.keras.api._v2.keras.layers' has no attribute 'AdditiveAttention'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-eddfef1668ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdditiveAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#procedure for score and distribution calculation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow.python.keras.api._v2.keras.layers' has no attribute 'AdditiveAttention'"
     ]
    }
   ],
   "source": [
    "#Additiveattention layer\n",
    "\n",
    "#method\n",
    "\n",
    "tf.keras.layers.AdditiveAttention(use_scale=True)\n",
    "\n",
    "#procedure for score and distribution calculation\n",
    "\"\"\"\n",
    "Reshape query and value into shapes [batch_size, Tq, 1, dim] and [batch_size, 1, Tv, dim] respectively.\n",
    "\n",
    "Calculate scores with shape [batch_size, Tq, Tv] as a non-linear sum: \n",
    "\n",
    "scores = tf.reduce_sum(tf.tanh(query + value), axis=-1)\n",
    "\n",
    "Use scores to calculate a distribution with shape [batch_size, Tq, Tv]: \n",
    "\n",
    "distribution = tf.nn.softmax(scores).\n",
    "\n",
    "Use distribution to create a linear combination of value with shape batch_size, Tq, dim]: \n",
    "\n",
    "return tf.matmul(distribution, value).\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
